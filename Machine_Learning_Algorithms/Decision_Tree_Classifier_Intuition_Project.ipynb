{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Tree Classifiers are binary trees that recursively splits a dataset until the leaf nodes are at their purest [Or represents one class]. They can help with classification tasks that involve non-linear decision boundaries.\n",
        "- There are two kinds of nodes; decision and leaf nodes. Decision nodes will split the data with a condition, while the leaf node helps decide the class of the data.\n",
        "- The root node holds all the data, while child nodes are the binary results of splitting. If a child node is pure, there is no need to split the node further.\n",
        "- There are obviously ways to calculate the optimal split condition to best classify the data and obtain the purest leaf notes, but how can the computer do the same?\n",
        "- Answer: **The model will choose the condition that has maximizes the information gain**\n",
        "\n",
        "- Information contained in a state:\n",
        "- **Entropy** can be used to measure the impurity in the data. If Entropy is high for a data split, we are very uncertain about the randomly picked point, and we need to split further. Essentially, the lower the entropy, the better the split is. The formula is as follows:\n",
        "\n",
        "      Σ-p_i * log_2(p_i); p_i is the probability of the i_th class [Will be multiple in the case of classification since it is a summation]\n",
        "  \n",
        "- To calculate information gain for a split, the model must subtract the combined entropies of the child node [multiplied by weights, or respective size of child in relation to the parent node] and the entropy of the parent node.\n",
        "\n",
        "    \n",
        "        IG = E(parent) - Σw_i * E(child_i)\n",
        "\n",
        "- The greater the info gain, the more likely it will be chosen as a split for the decision tree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0qDqfPg8WhVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply decision tree classifiers from scratch using python"
      ],
      "metadata": {
        "id": "kOAOpNEOaJei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "df = pd.DataFrame(iris.data)\n",
        "df.columns = ['sepal-length','sepal-width','petal-length','petal-width',]\n",
        "df['class'] = [0]*50 + [1]*50 + [2]*50\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtRRaBnX1PWC",
        "outputId": "4cf519a4-ab3a-4a4e-d0c5-a0f163a31095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal-length  sepal-width  petal-length  petal-width  class\n",
            "0             5.1          3.5           1.4          0.2      0\n",
            "1             4.9          3.0           1.4          0.2      0\n",
            "2             4.7          3.2           1.3          0.2      0\n",
            "3             4.6          3.1           1.5          0.2      0\n",
            "4             5.0          3.6           1.4          0.2      0\n",
            "..            ...          ...           ...          ...    ...\n",
            "145           6.7          3.0           5.2          2.3      2\n",
            "146           6.3          2.5           5.0          1.9      2\n",
            "147           6.5          3.0           5.2          2.0      2\n",
            "148           6.2          3.4           5.4          2.3      2\n",
            "149           5.9          3.0           5.1          1.8      2\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
        "        ''' constructor '''\n",
        "\n",
        "        # for decision node\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.info_gain = info_gain\n",
        "\n",
        "        # for leaf node\n",
        "        self.value = value\n",
        "\n",
        "\n",
        "class DecisionTreeClassifier():\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "        ''' constructor '''\n",
        "\n",
        "        # initialize the root of the tree\n",
        "        self.root = None\n",
        "\n",
        "        # stopping conditions\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        ''' recursive function to build the tree '''\n",
        "\n",
        "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
        "        num_samples, num_features = np.shape(X)\n",
        "\n",
        "        # split until stopping conditions are met\n",
        "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
        "            # find the best split\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"]>0:\n",
        "                # recur left\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # recur right\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return decision node\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "\n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        # return leaf node\n",
        "        return Node(value=leaf_value)\n",
        "\n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        ''' function to find the best split '''\n",
        "\n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        max_info_gain = -float(\"inf\")\n",
        "\n",
        "        # loop over all the features\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = dataset[:, feature_index]\n",
        "            possible_thresholds = np.unique(feature_values)\n",
        "            # loop over all the feature values present in the data\n",
        "            for threshold in possible_thresholds:\n",
        "                # get current split\n",
        "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
        "                # check if childs are not null\n",
        "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    # compute information gain\n",
        "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
        "                    # update the best split if needed\n",
        "                    if curr_info_gain>max_info_gain:\n",
        "                        best_split[\"feature_index\"] = feature_index\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"info_gain\"] = curr_info_gain\n",
        "                        max_info_gain = curr_info_gain\n",
        "\n",
        "        # return best split\n",
        "        return best_split\n",
        "\n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        ''' function to split the data '''\n",
        "\n",
        "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
        "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
        "        return dataset_left, dataset_right\n",
        "\n",
        "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
        "        ''' function to compute information gain '''\n",
        "\n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        if mode==\"gini\":\n",
        "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
        "        else:\n",
        "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
        "        return gain\n",
        "\n",
        "    def entropy(self, y):\n",
        "        ''' function to compute entropy '''\n",
        "\n",
        "        class_labels = np.unique(y)\n",
        "        entropy = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            entropy += -p_cls * np.log2(p_cls)\n",
        "        return entropy\n",
        "\n",
        "    def gini_index(self, y):\n",
        "        ''' function to compute gini index '''\n",
        "\n",
        "        class_labels = np.unique(y)\n",
        "        gini = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            gini += p_cls**2\n",
        "        return 1 - gini\n",
        "\n",
        "    def calculate_leaf_value(self, Y):\n",
        "        ''' function to compute leaf node '''\n",
        "\n",
        "        Y = list(Y)\n",
        "        return max(Y, key=Y.count)\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        ''' function to print the tree '''\n",
        "\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        if tree.value is not None:\n",
        "            print(tree.value)\n",
        "\n",
        "        else:\n",
        "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
        "            print(\"%sleft:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.left, indent + indent)\n",
        "            print(\"%sright:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.right, indent + indent)\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        ''' function to train the tree '''\n",
        "\n",
        "        dataset = np.concatenate((X, Y), axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "    def predict(self, X):\n",
        "        ''' function to predict new dataset '''\n",
        "\n",
        "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
        "        return preditions\n",
        "\n",
        "    def make_prediction(self, x, tree):\n",
        "        ''' function to predict a single data point '''\n",
        "\n",
        "        if tree.value!=None: return tree.value\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val<=tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)"
      ],
      "metadata": {
        "id": "fvI4-V-s1Yww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, :-1].values\n",
        "Y = df.iloc[:, -1].values.reshape(-1,1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
      ],
      "metadata": {
        "id": "7FQpCBNA8Uzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n",
        "classifier.fit(X_train,Y_train)\n",
        "classifier.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gveFL9lK8eIL",
        "outputId": "2b9bc0d7-0496-4ac7-a94a-34c672df21d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_2 <= 1.9 ? 0.33741385372714494\n",
            " left:0.0\n",
            " right:X_3 <= 1.5 ? 0.427106638180289\n",
            "  left:X_2 <= 4.9 ? 0.05124653739612173\n",
            "    left:1.0\n",
            "    right:2.0\n",
            "  right:X_2 <= 5.0 ? 0.019631171921475288\n",
            "    left:X_1 <= 2.8 ? 0.20833333333333334\n",
            "        left:2.0\n",
            "        right:1.0\n",
            "    right:2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(Y_test, Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4zdU07Y89PY",
        "outputId": "85d47287-212a-4daf-e07a-142004a98a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression for Decision Trees work the same, except we calculate variance reduction instead of entropy for measuring impurity. Variance reduction is:\n",
        "\n",
        "      Var = 1/nΣ(y_i - y_mean)^2\n",
        "And Variance Reduction [Essentially info gain] is calculated as:\n",
        "\n",
        "      Var_Red = Var(Parent) - Σw_iVar(Child_i) [ith child for a split, w_i being the size of child node w.r.t parent]\n",
        "[The greater the var reduction, the more likely we choose the split. Everything else remains]"
      ],
      "metadata": {
        "id": "_ieVfOZcDyC5"
      }
    }
  ]
}